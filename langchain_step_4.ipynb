{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObnkuioQLlkv9zxLKgPo7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suriyanand/GEN_AI_PROJECTS/blob/main/langchain_step_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwkwuK4KRrbY"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings , HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import chroma\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_loader = TextLoader(\"/content/example.txt\")\n",
        "text_docs = text_loader.load()\n",
        "print(text_docs)"
      ],
      "metadata": {
        "id": "U2xXMXSaRswn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5db6c9-21e6-45e0-cf0c-425a7b63b48d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'source': '/content/example.txt'}, page_content='LangChain is a framework for developing applications powered by language models. It enables the chaining of components like prompt templates, language model calls, and output parsers in a flexible and easy-to-use way.\\n\\nLangChain supports document loading, splitting, embedding, and storage using vector databases. This makes it suitable for building powerful retrieval-based applications like chatbots, Q&A systems, and search engines.\\n\\nDevelopers can use embeddings from providers like HuggingFace or OpenAI and persist data into vector stores like Chroma or FAISS for efficient retrieval.\\n')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loader = PyPDFLoader(\"https://arxiv.org/pdf/2305.12675.pdf\")\n",
        "pdf_docs = pdf_loader.load()\n",
        "print(pdf_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQsdCUy4zzhj",
        "outputId": "94678338-c076-453e-f140-dc12abf8c626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='A Frustratingly Simple Decoding Method for\\nNeural Text Generation\\nHaoran Yang♠,∗, Deng Cai♡,†, Huayang Li♣, Wei Bi♡, Wai Lam♠, Shuming Shi♡\\n♠The Chinese University of Hong Kong ♡Tencent AI Lab ♣Nara Institute of Science and Technology\\n{hryang, wlam}@se.cuhk.edu.hk, li.huayang.lh6@is.naist.jp\\n{jcykcai, victoriabi, shumingshi}@tencent.com\\nAbstract\\nWe introduce a frustratingly simple, highly efficient, and surprisingly effective decoding method, termed Frustratingly\\nSimple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: We construct an\\nanti-language model (anti-LM) based on previously generated text, which is employed to penalize the future\\ngeneration of repetitive content. The anti-LM can be implemented as simple as an n-gram language model\\nor a vectorized variant. In this way, FSD incurs no additional model parameters and negligible computational\\noverhead (FSD can be as fast as greedy search). Despite its simplicity, FSD is surprisingly effective and generalizes\\nacross different datasets, models, and languages. Extensive experiments show that FSD outperforms established\\nstrong baselines in terms of generation quality, decoding speed, and universality. The code is available at\\nhttps://github.com/LHRYANG/FSD\\nKeywords:language model, decoding method, universality, efficiency\\n1. Introduction\\nNeural text generation has attracted increasing\\nattention from both academia and industry. The\\ncanonical approach factors the generation process\\nin an autoregressive fashion, reducing the gener-\\nation into a series of next-token predictions condi-\\ntioned on their preceding sequences. With the de-\\nvelopment of large language models (LMs) (Brown\\net al., 2020; Touvron et al., 2023a,b), the estima-\\ntion of the probability distribution for next-token pre-\\ndictions has become remarkably accurate. How-\\never, when it comes to open-ended text genera-\\ntion, such as story generation (Fan et al., 2018)\\nand writing assistance (Shi et al., 2022), perhaps\\ncounter-intuitively, searching for the most likely se-\\nquences (e.g., greedy search and beam search)\\noften results in low-quality outputs. Concretely, the\\ngenerations are prone to falling into tedious and\\nrepetitive loops, a notorious issue referred to as\\nneural text degeneration (Holtzman et al., 2020;\\nXu et al., 2022; Shi et al., 2024).\\nTo address the above problem, two lines of re-\\nsearch efforts have been devoted to devising better\\ndecoding strategies. The canonical approaches\\ntake random samples from the LM’s output dis-\\ntribution (Fan et al., 2018; Holtzman et al., 2020;\\nMeister et al., 2022; Hewitt et al., 2022). The intro-\\nduced stochasticity can alleviate repetitive genera-\\ntion, however, it also increases the chance of un-\\nnatural topic drift and semantic incoherence. More\\nrecently, another class of approaches proposes\\nto re-rank top candidate tokens using extra ob-\\n∗Most work was done during an internship at Ten-\\ncent AI Lab.\\n†Corresponding author.\\nwearing          \\n…\\nnot      \\n……\\ndriving      not      \\n…\\n…\\n…\\nwearing          \\nThe man is driving the car on the snow. The man is … \\nupdate\\nGreedy:    driving the car on the snow.\\nFSD:    wearing a white shirt and black pants.\\ndriving      \\nanti-LMLM\\nnot      \\n… ……\\nwearing          \\ndriving      \\nprefix \\noutput  \\nFigure 1: FSD exploits the contrasts between the\\nLM and the anti-LM, where the probabilities from\\nthe LM and the anti-LM are used as rewards and\\npenalties respectively. In the above example, the\\ntop prediction of the LM is “driving”. However, the\\nanti-LM also gives a large penalty to “driving\" be-\\ncause it will result in repetition. Consequently,\\n“wearing” is instead selected and the anti-LM is\\nupdated accordingly.\\njectives. Concretely, contrastive search (CS) (Su\\net al., 2022) uses a look-ahead mechanism and\\npenalizes tokens compromising the isotropy of the\\nLM’s latent space (Ethayarajh, 2019). Contrastive\\ndecoding (CD) (Li et al., 2023a) searches for the\\ntoken that maximizes the probability difference be-\\ntween the LM and another smaller LM with the\\nsame tokenization. Despite better generation qual-\\nity is achieved, the look-ahead mechanism in CS\\narXiv:2305.12675v2  [cs.CL]  27 Feb 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content='and the running of an external LM in CD consider-\\nably increase computational overhead. Moreover,\\nCS relies on the isotropic property of the LM and\\nCD depends on another LM using the same tok-\\nenization, thereby limiting their applicability.\\nIn this paper, we propose Frustratingly Simple\\nDecoding (FSD) for addressing the degeneration\\nissue with minimal computational cost and without\\nany assumptions on the underlying LM. As illus-\\ntrated in Figure 1, FSD works by imposing penal-\\nties on repetitive patterns that have appeared in the\\nprefix. This is realized through an anti-LM that can\\ncapture and memorize these patterns. Specifically,\\nat each generation step, both the LM and the anti-\\nLM take the current prefix as input and separately\\nproduce two next-token distributions. The genera-\\ntion probabilities from the LM serve as rewards and\\nthose from the anti-LM act as penalties. FSD sub-\\ntracts the penalties from the rewards, selects the\\ntoken that maximizes the final score, and continu-\\nously updates the anti-LM based on the growing\\nprefix. The anti-LM can be implemented as sim-\\nple as an n-gram language model or a vectorized\\nvariant, making FSD as fast as greedy search.\\nWe perform extensive experiments to demon-\\nstrate the effectiveness, efficiency, and universality\\nof FSD. The key findings can be summarized as\\nfollows: (1) On three canonical open-ended text\\ngeneration benchmarks, the generation quality of\\nFSD not only surpasses the standard top- p sam-\\npling but also is comparable to, if not better than,\\nrecent state-of-the-art methods, according to both\\nautomatic and human evaluations. (2) FSD exhibits\\nrobustness in handling varying generation lengths,\\nparticularly demonstrating its superiority in gen-\\nerating longer sequences where existing state-of-\\nthe-art methods often struggle. (3) The generation\\nspeed of FSD is as fast as greedy search (the theo-\\nretical upper bound for autoregressive generation).\\nThe speed advantage over existing state-of-the-\\nart methods amplifies as the generation length in-\\ncreases. (4) FSD shows versatility across a variety\\nof models, languages, and tasks (e.g., instruction\\nfollowing and summarization).\\n2. Related Work\\nRecent years have witnessed enormous progress\\nin neural text generation, particularly with the suc-\\ncess of large LMs (Radford et al., 2019). The most\\nstraightforward heuristics for generating text from\\nan LM is to find the most likely sequence estimated\\nby the LM. Although maximizing the LM probabil-\\nities (e.g., greedy search and beam search) ob-\\ntains excellent performance in close-ended text\\ngeneration tasks (e.g., translation (Sutskever et al.,\\n2014) and summarization (See et al., 2017)), these\\nsearch-based methods suffer from generating non-\\nsensical output in open-ended text generation\\ntasks (e.g., story generation (Fan et al., 2018)).\\nOne prominent issue is that they tend to generate\\ndull and repetitive output (Holtzman et al., 2020;\\nFu et al., 2021; Pillutla et al., 2021).\\nDecoding Methods To tackle the above chal-\\nlenge, different decoding methods have been pro-\\nposed, which can be broadly categorized into two\\nclasses. The first class is truncated sampling,\\nwhere each token is randomly sampled from a\\ntruncated next-token distribution. For instance, top-\\nk sampling (Fan et al., 2018) only samples from\\nthe k most likely tokens. Top- p sampling (Holtz-\\nman et al., 2020) only considers the minimal set of\\ntop tokens that cover a specified percentage p of\\nthe distribution. Typical sampling (Meister et al.,\\n2022) sorts tokens according to the differences be-\\ntween distribution entropy and probabilities. Hewitt\\net al. (2022) truncate words whose probabilities are\\nbelow an entropy-dependent threshold. Although\\nsampling-based methods reduce repetitions, the\\nrandomness at each sampling step also increases\\nthe chance of incoherence and topic drift.\\nThe second class of decoding methods is still\\nsearch-based but optimizes a different objective.\\nContrastive Search (CS) (Su et al., 2022) assumes\\nthe LM has an isotropic representation space and\\nadds a penalty term that decreases the generation\\nprobabilities of tokens producing hidden states that\\nare similar to the previous context. However, the\\nlook-ahead operation at each step brings consider-\\nable additional cost. Contrastive Decoding (CD) (Li\\net al., 2023a) employs an amateur LM (a smaller\\npre-trained LM using the same tokenization) and\\npenalizes undesired attributes associated with the\\namateur model. In contrast, FSD is much more\\nlightweight and efficient; FSD only constructs an n-\\ngram model on-the-fly, requiring no external model\\nand introducing negligible computational cost. In\\naddition, FSD holds the potential for broader appli-\\ncability as it does not assume the existence of an\\namateur LM or any properties of the LM.\\nTraining Methods Another group of methods\\nattempts to improve text generation quality by\\nfine-tuning the LMs with new training objectives.\\nWelleck et al. (2020) propose unlikelihood training,\\nwhich explicitly minimizes the generation probabil-\\nity of repetitive tokens. Lagutin et al. (2021) im-\\nprove the generation using policy gradient with a\\nrepetition objective. Xu et al. (2022) learn to penal-\\nize probabilities of sentence-level repetitions from\\npseudo-repetitive data. Su et al. (2022) devise\\na contrastive training objective that encourages\\ndiscriminative and isotropic token representations.\\nIn contrast, FSD simply employs off-the-shelf pre-\\ntrained LMs and requires zero training.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content='3. Background\\n3.1. Language Models\\nAn LM is a probability distribution over token se-\\nquences. Given a sequence x1:t = x1, x2, . . . , xt\\nof length t, LM assigns a probability p(x1:t) to the\\nsequence, which is usually decomposed in an au-\\ntoregressive fashion: p(x1:t) =Qt\\ni=1 p(xi|x<i).\\nN-gram Language Model The most traditional\\nLM is then-gram model, which relies on the Markov\\nassumption (Jurafsky and Martin, 2009). In an\\nn-gram LM, the probability of thei-th token only de-\\npends on the previous n − 1 tokens, expressed as\\np(xi|x<i) =pn(xi|xi−n+1:i−1). This probability can\\nbe computed by evaluating the relative frequency\\ncounts within a training corpus:\\npn(xi|xi−n+1:i−1) = C(xi−n+1:i)\\nC(xi−n+1:i−1) (1)\\nwhere C(·) counts the number of occurrences\\nof the input sequence within the training corpus.\\nIn practice, the probability distributions are often\\nsmoothed to improve the model’s generalizability.\\nFor example, the interpolation of n-gram models\\nof different orders can help prevent the LM from\\nassigning zero probability to unseen sequences\\n(Tonella et al., 2014).\\nNeural Language Model With the rise of deep\\nlearning, n-gram LMs have been largely super-\\nseded by neural networks, for example, the GPT\\nfamily (Radford et al., 2019; Brown et al., 2020)\\nand the LLaMA family (Touvron et al., 2023a,b).\\nThese models are trained to predict the next token\\nby conditioning on the preceding context: Lθ =\\n−Pt\\ni=1 log pθ(xi|x<i), where θ denotes the model\\nparameters. With the capabilities acquired by large-\\nscale pre-training, these neural LMs can be readily\\napplied to text generation (Liu et al., 2022).\\n3.2. Open-Ended Text Generation\\nMost of our experiments are conducted on open-\\nended text generation tasks, where the input is\\na short prompt and the goal is to generate a flu-\\nent and coherent continuation. Formally, given\\na prompt x1:l = x1, x2, . . . , xl, we aim to gener-\\nate the next m tokens, denoted by xl+1:l+m =\\nxl+1, xl+2, . . . , xl+m. A pre-trained neural LM can\\ncomplete this task autoregressively by a series of\\nnext-token predictions:\\npθ(xl+1:l+m|x1:l) =\\nl+mY\\ni=l+1\\npθ(xi|x<i)\\nPrevious works have revealed that the decoding\\nmethod that selects the token at each generation\\nstep has a significant impact on the generation\\nquality (Holtzman et al., 2020; Wiher et al., 2022).\\nFor example, greedy and beam search often result\\nin repetitions while sampling-based methods suffer\\nfrom incoherence (Su et al., 2022; Li et al., 2023a).\\n4. Method\\nWe present our proposed decoding method, Frus-\\ntratingly Simple Decoding (FSD), named after its\\nremarkably straightforward nature. We begin by\\nintroducing the intuition and the general framework\\nof FSD (§4.1). We then describe the implemen-\\ntation of FSD in the discrete version (§4.2) and\\nfurther extend it to the vectorized version (§4.3).\\n4.1. Intuition & Framework\\nTo produce coherent and diverse generations, it\\nis crucial not only to select the most probable to-\\nkens but also to prevent repetitive content. While\\nthe former objective can be achieved using the\\noriginal LM, the latter requires a mechanism for\\ntracking previously generated content and reduc-\\ning their likelihood of reoccurrence. To this end, we\\npropose the construction of an anti-LM based on\\nthe preceding context. This anti-LM is expected\\nto assign higher scores to tokens that will cause\\nrepetitions in the preceding context. Consequently,\\nthese scores serve as penalties. By integrating\\nthe original LM and the anti-LM, we can discour-\\nage repetitive token generation and promote other\\ncontextually appropriate choices.\\nFormally, when decoding the t-th token, we cal-\\nculate an FSD score for each candidate token v:\\nFSD(v|x<t) =pθ(v|x<t) − α × pω(v|x<t) (2)\\nwhere pθ and pω represent the LM and the anti-LM\\nrespectively. The hyper-parameter α ≥ 0 is used\\nto balance the two scores. In practice, we first\\nselect the top-k most probable tokens according\\nto pθ(·|x), denoted by V (k). The token in V (k) with\\nthe largest FSD score is chosen as the t-th token.\\n4.2. N-gram Model as anti-LM\\nFollowing the intuition described above, we start\\nto devise the anti-LM. In principle, any language\\nmodel capable of capturing patterns in a token\\nsequence can be harnessed to implement the anti-\\nLM. However, we note several critical design princi-\\nples. First, the prediction of the anti-LM should be\\nefficient, given that it is invoked at every decoding\\nstep. Second, the anti-LM should not assume any\\nparticular properties of the LM or the language,\\nthus ensuring our method’s universal applicability\\nacross diverse settings. Last but not least, the up-\\ndate of the anti-LM should be easy, as it undergoes\\ncontinuous evolution with the expanding prefix.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Algorithm 1:Calculation of Penalty pω(v|x<t)\\nInput : prefix x<t; n-gram models with different\\norders from 1 to N (p1, p2, ··· pN );\\ncandidate token v; decay factor β = 0.9\\n1 Initialize r = 1, cv = 0\\n2 for n = N, ··· , 2 do\\n3 if pn(v|x<t) ̸= 0then\\n4 λn = r ∗ β\\n5 r = r − λi\\n6 cv += λnpn(v|xt−n+1:t−1)\\n7 cv += rp1(v)\\nOutput :pω(v|x<t) =cv\\nOne natural (and perhaps the simplest) choice\\nis the n-gram LM, which offers two key advantages.\\nFirst, all the operations (i.e., construction, predic-\\ntion, and update) associated with an n-gram model\\nadd little computational overhead. Second, the ef-\\nfectiveness and efficiency ofn-gram LM is scalable\\nacross different prefix lengths.\\nConstruction and Update Given an input\\nprompt x1:l, the n-gram LM is constructed and up-\\ndated as follows. Initially, the promptx1:l is split into\\nn-grams. These n-grams can be stored as a set of\\nkey-value pairs Dn. For each n-gram xi−n+1:i, the\\nkey is the firstn−1 tokens xi−n+1:i−1 and the value\\nis the last token xi. After generating each new\\ntoken, we update Dn to include the new n-gram\\ncomposed by the last n tokens in the sequence.\\nTo calculate next-token probabilities, we use the\\nlast n −1 tokens in the sequence as the query. We\\nfirst identify all key-value pairs in Dn whose key\\nprecisely matches the query and then compute the\\nprobabilities according to Eq. 1. All of the above\\noperations introduce little computational overhead\\ncompared to the running of the original neural LM.\\nSmoothed N-gram Model An ordinary n-gram\\nmodel cannot penalize the m(m < n)-gram repe-\\ntitions. Inspired by two common smoothing tech-\\nniques in modern n-gram models, back-off and\\ninterpolation (Jurafsky and Martin, 2009), we com-\\nbine n-gram models with different orders from\\nn = 1to N (N being the highest order). The result\\nis a smoothed n-gram model ˆp:\\nˆp = λN pN + λN−1pN−1 + ··· + λ1p1 (3)\\nwhere λn is the weight ofpn and PN\\nn=1 λn = 1. The\\ndetailed process is elaborated in Alg. 1. In brief, we\\nenumerate n-gram models from n = N to n = 1,\\nsetting λn to decrease exponentially with a decay\\nfactor β = 0.9, thus assigning greater weights to\\nhigher-order sub-models. The construction and\\nupdate of the smoothed n-gram LM are straight-\\nforward; We only need to maintain N copies of\\nkey-value pairs (D1, D2, . . . ,DN ) separately.\\n4.3. Vectorized N-gram Model\\nWe further provide a vectorized version where the\\nkeys are represented using continuous vectors in-\\nstead of discrete tokens. It offers two advantages\\ncompared with the discrete version. First, it pos-\\nsesses the ability to penalize not only identical but\\nalso similar patterns in the preceding context, thus\\nallowing for more generalizable pattern recognition.\\nSecond, the computation of the vectorized version\\ncan be efficiently conducted on GPU, resulting in\\nfaster decoding speed.\\nSpecifically, we use the hidden states from the\\nlast layer of the original LM as the keys. Let\\nh1, h2, . . . ,ht−1 be the hidden states for the cur-\\nrent sequence x1:t−1 (ht−1 is used to predict the\\nt-th token in the original LM). Each key-value pair\\nin the discrete version (xi−n+1:i−1, xi) now turns to\\nbe (hi−n+1:i−1, xi). Accordingly, the exact query-\\nkey matching in the discrete version becomes a\\n“soft” vector matching. To predict the next token,\\nthe query is ht−n+1:t−1 and the matching score be-\\ntween the query and a key hi−n+1:i−1 is computed\\nas follows:\\nci = cos(cat(hi−n+1:i−1), cat(ht−n+1:t−1)) (4)\\nwhere cos computes cosine similarity and cat de-\\nnotes vector concatenation. For a candidate token\\nv that appears multiple times in the sequence, we\\ntake the largest matching score as its penalty score.\\nIn addition, we clip the penalty score to ensure it is\\nalways greater than or equal to zero.\\n5. Experiments\\nOur main experiments focus on open-ended text\\ngeneration. This task has been used for evaluating\\nvarious decoding methods in recent works (Li et al.,\\n2023a; Su et al., 2022; Lan et al., 2022) because\\nit is particularly susceptible to the repetition issue.\\nWe follow the standard setups (§5.1) and report the\\nresults in §5.2. In addition, we assess the speed of\\nthe decoding methods in §5.3, an essential aspect\\nwhen considering real-world deployment. More-\\nover, we explore the universality of our proposed\\nmethod in §5.4 from several perspectives: (1) ro-\\nbustness across various models, languages, and\\ndatasets (2) versatility for tackling other tasks such\\nas instruction following (the most popular use of\\nLLMs) and closed-ended generation.\\n5.1. Setup for Open-Ended Text\\nGeneration\\nDatasets & Models Following previous works\\n(Su et al., 2022; Li et al., 2023a; Lan et al., 2022),\\nwe compare FSD and existing decoding methods'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='wikinews wikitext book\\ndiv mau coh div mau coh div mau coh\\nHuman 0.92 1 0.65 0.93 1 0.63 0.95 1 0.51\\nOPT -6.7b\\np=0.95 0.91 0.95 0.60 0.87 0.95 0.59 0.93 0.92 0.48\\ntypical=0.95 0.94 0.93 0.58 0.93 0.93 0.56 0.95 0.89 0.45\\nCS 0.91 0.93 0.62 0.87 0.91 0.57 0.86 0.88 0.47\\nCD 0.93 0.95 0.69 0.89 0.95 0.69 0.87 0.95 0.61\\nFSD 0.95 0.93 0.66 0.94 0.93 0.61 0.95 0.85 0.51\\nFSD-vec 0.95 0.93 0.64 0.92 0.90 0.60 0.96 0.87 0.49\\nGPT2-XL\\np=0.95 0.94 0.96 0.60 0.92 0.94 0.57 0.94 0.95 0.46\\ntypical=0.95 0.95 0.93 0.56 0.95 0.92 0.53 0.96 0.87 0.43\\nCS 0.93 0.92 0.64 0.86 0.92 0.60 0.88 0.89 0.48\\nCD 0.92 0.92 0.69 0.89 0.93 0.69 0.83 0.93 0.64\\nFSD 0.93 0.93 0.66 0.94 0.88 0.61 0.96 0.90 0.49\\nFSD-vec 0.93 0.93 0.64 0.93 0.90 0.58 0.96 0.91 0.47\\nGPT2-Medium\\np=0.95 0.96 0.94 0.56 0.96 0.92 0.53 0.97 0.92 0.43\\ntypical=0.95 0.96 0.94 0.56 0.96 0.93 0.53 0.97 0.91 0.43\\nCS 0.03 0.14 0.65 0.02 0.07 0.64 0.01 0.03 0.50\\nCD 0.88 0.95 0.71 0.83 0.88 0.71 0.68 0.92 0.67\\nFSD 0.94 0.93 0.65 0.94 0.91 0.60 0.97 0.87 0.49\\nFSD-vec 0.94 0.90 0.60 0.92 0.86 0.55 0.93 0.92 0.44\\nTable 1: Automatic evaluation results. The best results (the closer to human the better) are boldfaced.\\non three English benchmarks. That is, wikinews1 in\\nthe news domain, wikitext-103 (Merity et al., 2017)\\nin the Wikipedia domain and bookcorpus (Zhu\\net al., 2015) in the story domain. For each test\\ncase, the first 32 tokens are used as the prompt\\nand the task is to generate the following 256 to-\\nkens. We test three off-the-shelf LMs of different\\nscales: OPT -6.7b (Zhang et al., 2022), GPT2-XL,\\nand GPT2-Medium (Radford et al., 2019). The am-\\nateur LM used in CD is OPT -125m for OPT -6.7b\\nand GPT2 for GPT2-XL and GPT2-Medium.\\nEvaluation Metrics For automatic evaluation, we\\nreport three metrics assessing different aspects of\\nthe generations:\\n• Diversity measures the degree of repetition at\\ndifferent n-gram levels. The calculation can be\\nexpressed as\\n4Q\\nn=2\\n(1 − REPn), where REPn = (1−\\n#unique n-grams(ˆx)\\n#total n-grams(ˆx) ). ˆx is the generated continuation.\\nA higher diversity score indicates that generated\\noutputs contain fewer repetitive segments.\\n• MAUVE (Pillutla et al., 2021) measures the distri-\\nbution similarity between the generated texts and\\nreference texts.\\n• Coherence (Su et al., 2022) is defined as the\\ncosine similarity between the embeddings of the\\nprompt x and the generated continuation ˆx: COH =\\nf(x)f(ˆx)\\n∥f(x)∥∥f(ˆx)∥, where f is the SimCSE (Gao et al.,\\n2021) sentence embedding function.\\n1http://www.wikinews.org\\nFor human evaluation, we conduct blind A/B\\ntests with the help of proficient English speakers\\nfrom a third-party grading platform. In the process\\nof annotation, annotators are asked to compare\\ntwo continuations of the same prompt and decide\\nwhich one is better (or two are equally good/bad)\\nby jointly considering fluency, coherence, and com-\\nmonsense. Each case is rated by three annotators\\nand we use majority vote.\\nImplementation Details For clarity, the variant of\\nFSD using the vectorized n-gram model is named\\nas FSD-vec. We set n to 3 and 2 for FSD and\\nFSD-vec respectively and k to 6 for both variants.\\nBased on our preliminary experiments, the penalty\\nstrength α is set to 3 and 1 for FSD and FSD-vec\\nrespectively. We find this setting is quite robust\\nand generalizes well to different scenarios.\\nBaselines To show the superior performance of\\nFSD/FSD-vec, we mainly compared it with two re-\\ncent search-based decoding methods, CD (Li et al.,\\n2023a) and CS (Su et al., 2022), since they were re-\\nported to outperform other existing decoding meth-\\nods.2 We follow the suggested hyper-parameter\\nsettings from their respective papers. We also com-\\npare with top-p sampling (Holtzman et al., 2020)\\nbecause it is the most popular decoding method\\nfor open-ended text generation. We also include\\n2We omit greedy and beam search due to space\\nlimitation. The text generated by these two methods is\\nvery repetitive and of low quality (Li et al., 2023a).'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content='the results of typical sampling (Meister et al., 2022).\\nWe set p in top-p sampling and typical sampling to\\n0.95, as adopted by Li et al. (2023a).\\n5.2. Main Results\\nAutomatic Evaluation Results For automatic\\nmetrics, we believe that results closer to human\\nare better because a higher score does not always\\nindicate a better generation. For example, a ran-\\ndom token sequence would obtain an extremely\\nhigh diversity score, and a continuation identical to\\nthe input prompt would get a full coherence score.\\nThis is also commonly adopted in previous works\\n(Holtzman et al., 2020; Meister et al., 2022; Xu\\net al., 2022). Therefore, we highlight the results\\nthat are closest to human in our experiments. From\\nTable 1, we can observe that:\\n• For diversity (div), FSD/FSD-vec matches or out-\\nperforms all other decoding baselines in six/five out\\nof nine settings (the combinations of three LMs and\\nthree domains). In cases where FSD and FSD-vec\\nare not the best, the gaps between them and the\\nbest scores are minimal (< 0.03). It is worth not-\\ning that recent state-of-the-art methods (CD and\\nCS) are very sensitive to the choices of the LMs.\\nFor example, CS fails to achieve reasonable diver-\\nsity scores on all three benchmarks when using\\nGPT2-Medium. The reason is that CS relies on\\nthe isotropy of the LM’s latent space and GPT2-\\nMedium may not fulfill this requirement. The diver-\\nsity scores of CD also decrease significantly as the\\nLM switches from GPT2-XL to GPT2-Medium, per-\\nhaps because the difference between the LM and\\nits amateur is not sufficiently indicative of degen-\\neration. In contrast, FSD and FSD-vec are much\\nmore stable in diversity. We attribute the success\\nto that the operations of the anti-LM in FSD are\\nrelatively independent to the choice of the LM.\\n• For coherence (coh), FSD/FSD-vec achieves the\\nbest coherence scores in seven/four out of nine set-\\ntings. These results emphasize the effectiveness\\nof FSD and FSD-vec in generating coherent and\\ncontextually-appropriate continuations. We can\\nsee that sampling-based methods (top-p and typi-\\ncal sampling) often deliver lower coherence scores\\nthan search-based methods (CS and CD). This\\nconfirms that sampling-based methods produce\\nlexically diverse text at the expense of topic drift.\\nImportantly, FSD and FSD-vec often attain better\\ndiversity and better coherence at the same time,\\nsuggesting our methods provide a better trade-off\\nbetween diversity and coherence.\\n• For MAUVE ( mau), sampling-based methods\\n(particularly top-p sampling) are generally better\\nthan search-based methods (CS, CD, FSD, and\\nFSD-vec) though the gaps are often very small.\\nHowever, it has been reported that the generation\\nquality of CS and CD is better according to human\\nevaluation. This indicates that MAUVE may not be\\na reliable metric which is also pointed out by Su\\nand Xu (2022). Therefore, we turn to extensive\\nhuman evaluation.\\nHuman Evaluation Results For human evalua-\\ntion, we randomly select 100 prompts from each\\nof the three benchmarks. We first compare FSD\\nagainst top-p sampling and two recent state-of-the-\\nart methods, CS and CD. The results are shown in\\nTable 2. We can see that on average across set-\\ntings, annotators prefer FSD 1.30x more than CD,\\n1.26x more than top-p sampling and 1.14x more\\nthan CS. FSD wins all the comparisons with the\\nonly exception: FSD vs CS on book. The results\\nshow that CS is the most competitive method, we\\nthen turn to compare FSD-vec with CS and FSD.\\nAs shown in Table 3, FSD-vec wins all the compar-\\nisons against CS and is preferred 1.49x more than\\nCS. The quality of FSD-vec is on par with FSD.\\nCase Study We find that, compared with CS,\\nFSD is less likely to generate continuations that\\ndeviate from the topic. Table 4 shows two continu-\\nations from CS and FSD respectively. The prefix’s\\ntopic is “a musician is considering running for pres-\\nidency”. But the topic of CS’s output is concert\\ntours which is irrelevant to that of the prefix. It may\\nbe because CS tends to excessively penalize to-\\nkens in comparison to FSD. For instance, CS has\\nthe potential to penalize tokens that have never\\noccurred in the preceding context, as long as they\\nproduce similar hidden states. In contrast, FSD\\nonly penalizes tokens that appear in the context\\nand genuinely result in repetitions.\\nEffect of Decoding Length Next, we investi-\\ngate the robustness of our methods in address-\\ning the degeneration issue under different genera-\\ntion lengths. In Figure 2, we present the diversity\\nscores of FSD, FSD-vec, CS and CD when the gen-\\neration length is 256, 512 and 768. As seen, the\\ndiversity of human-generated text is most stable\\nacross different lengths. The diversity of CS and\\nCD drops dramatically as the generation length in-\\ncreases, resulting in a progressively larger disparity\\nbetween the generated text and human-generated\\ntext. In contrast, FSD has the smallest slope and\\nFSD-vec exhibits a similar slope to FSD from 256\\nto 512, and slightly steeper from 512 to 768. It\\nreveals that our method is much more robust in re-\\nducing repetitions in longer sequence generation.\\n5.3. Decoding Speed\\nTo compare the decoding speed of different meth-\\nods, we plot the decoding latency (seconds per in-\\nstance) of search-based methods in Figure 3. For'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='wikinews\\nA is better Neutral B is better\\nFSD 41% 22% 37% top- p\\nFSD 45%† 25% 30% CS\\nFSD 52%† 12% 36% CD\\nwikiext\\nA is better Neutral B is better\\nFSD 46%† 24% 30% top- p\\nFSD 39% 24% 37% CS\\nFSD 37% 30% 33% CD\\nbook\\nA is better Neutral B is better\\nFSD 41% 24% 35% top- p\\nFSD 38% 22% 40% CS\\nFSD 46%† 19% 35% CD\\nTable 2: Human evaluation results of FSD.† means\\nthe advantage is statistically significant as judged\\nby Sign Test withp-value< 0.05.\\nwikinews\\nA is better Neutral B is better\\nFSD-vec 44%† 25% 31% CS\\nFSD-vec 36% 21% 43% FSD\\nwikiext\\nA is better Neutral B is better\\nFSD-vec 51%† 26% 23% CS\\nFSD-vec 36%† 33% 31% FSD\\nbook\\nA is better Neutral B is better\\nFSD-vec 42% 20% 38% CS\\nFSD-vec 37% 27% 36% FSD\\nTable 3: Human evaluation results of FSD-vec. †\\nmeans the advantage is statistically significant as\\njudged by Sign Test withp-value< 0.05.\\nclarity, we omit the results of sampling-based meth-\\nods because they are close to greedy search. We\\ncan see that both FSD and FSD-vec demonstrate\\nsuperior decoding speed compared with CD and\\nCS, being more than 1.5x faster. In fact, FSD and\\nFSD-vec can match the speed of greedy search.\\nThis can be attributed to the minimal computational\\noverhead brought by the n-gram anti-LM, as op-\\nposed to the time-consuming look-ahead mech-\\nanism in CS and the running of an amateur LM\\nin CD. Importantly, as the generation length in-\\ncreases, the absolute speed gap between FSD\\nand CS/CD becomes even more pronounced, in-\\ncreasing from 8/10 seconds to 20/40 seconds per\\ninstance. This highlights the great efficiency advan-\\ntage of our methods in generating long sequences.\\nNote that FSD-vec is slightly faster than FSD. The\\nreason is that the computation of the vectorized\\nn-gram can be efficiently performed on GPUs.\\n5.4. Universality\\nMore Languages, Models and DatasetsSo far,\\nour evaluation has been primarily focused on En-\\n/uni00000015/uni00000018/uni00000019/uni00000018/uni00000014/uni00000015/uni0000001a/uni00000019/uni0000001b\\n/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\\n/uni00000013/uni00000011/uni0000001a\\n/uni00000013/uni00000011/uni0000001b\\n/uni00000013/uni00000011/uni0000001c\\n/uni00000014/uni00000011/uni00000013/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c\\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051\\n/uni00000029/uni00000036/uni00000027\\n/uni00000029/uni00000036/uni00000027/uni00000010/uni00000059/uni00000048/uni00000046\\n/uni00000026/uni00000027\\n/uni00000026/uni00000036\\nFigure 2: Diversity across different generation\\nlengths.\\n/uni00000015/uni00000018/uni00000019/uni00000018/uni00000014/uni00000015/uni0000001a/uni00000019/uni0000001b\\n/uni00000027/uni00000048/uni00000046/uni00000052/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\\n/uni00000018\\n/uni00000015/uni00000013\\n/uni00000017/uni00000013\\n/uni00000019/uni00000013/uni0000002f/uni00000044/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000056/uni0000000c\\n/uni00000026/uni00000036\\n/uni00000026/uni00000027\\n/uni00000029/uni00000036/uni00000027\\n/uni00000029/uni00000036/uni00000027/uni00000010/uni00000059/uni00000048/uni00000046\\n/uni0000002a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c\\nFigure 3: Decoding latency tested on GPT2-XL.\\nglish corpora, and the types of LMs used are also\\nlimited. We here expand our evaluation to include\\nother non-English languages using various LMs.\\nWe conduct experiments on four datasets, chinese-\\nwiki 3, japanese-news 4, german-wiki, and french-\\nwiki 5. We adopt a variety of popular LMs, including\\nBLOOM-7b (BigScience, 2023), LLaMA-7b (Tou-\\nvron et al., 2023a), OPT -6.7b (Zhang et al., 2022).\\nThe evaluation results are shown in Table 5, where\\nalso report the results of the state-of-the-art decod-\\ning methods, CS and top-p (the missing positions\\nindicate the LM does not support the language.).\\nAs seen, FSD and FSD-vec generally outperform\\nCS and top-p (most of the boldfaced numbers are\\nfrom FSD and FSD-vec.). It should be noted that\\nfor BLOOM-7b, CS does not work entirely in all four\\nlanguages (see the extremely low diversity scores).\\nAdditionally, the performance of CS also exhibits\\ngreater sensitivity to different languages. For in-\\nstance, when applied to the japanese dataset using\\nOPT, the diversity and MAUVE scores are notably\\nlow. In contrast, FSD and FSD-vec deliver much\\nmore stable performance across different settings,\\n3https://github.com/SigmaQuan/\\nAwesome-Chinese-Corpus-Datasets-and-Models\\n4https://www.kaggle.com/datasets/tanreinama/\\njapanese-fakenews-dataset\\n5https://huggingface.co/datasets/wikipedia'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Prompt: Following months of rumours, musician Wyclef Jean today confirmed that he is considering standing for\\nthe presidency of Haiti in the country’s November elections. A statement from\\nCS: Jean’s publicist reads: \"Wyclef will make a decision on his political future in the coming days and is consulting\\nwith his family and advisors. \"The announcement comes after Jean, who has been touring the world in support of\\nhis latest album A New Day In The Old Town, was forced to cancel a concert in the Dominican Republic last week\\ndue to security concerns. He had been due to play the concert in Santo Domingo on November 4 but the concert\\nwas called off after Dominican authorities raised concerns about the safety of the concert venue, which is close to\\nthe border with Haiti. ···\\nFSD: Jean’s management company, La Nuit Productions, said: \"Wyclef is currently in the process of gathering\\ninformation and will make a decision in the coming days.\"Jean, who has been a vocal supporter of the Haitian\\ngovernment, has previously said he would consider running for the position. The musician, who is also a member\\nof the hip-hop group A Tribe Called Quest, has not yet announced his candidacy. Haiti’s current president, Michel\\nMartelly, is widely expected to run for re-election. ···\\nTable 4: Case study: FSD vs CS.\\ngerman french japanese chinese\\ndiv mau coh div mau coh div mau coh div mau coh\\nHuman 0.93 1 0.77 0.88 1 0.65 0.56 1 0.60 0.75 1 0.87\\nBLOOM\\np=0.95 0.93 0.45 0.69 0.90 0.95 0.63 0.56 0.08 0.51 0.72 0.82 0.86\\nCS 0.20 0.72 0.72 0.50 0.90 0.64 0.04 0.23 0.53 0.07 0.57 0.80\\nFSD 0.93 0.79 0.76 0.90 0.90 0.67 0.59 0.11 0.53 0.75 0.75 0.84\\nFSD-vec 0.92 0.73 0.74 0.91 0.91 0.65 0.55 0.06 0.50 0.80 0.75 0.83\\nOPT\\np=0.95 0.91 0.70 0.73 0.89 0.70 0.60 0.73 0.61 0.55 - - -\\nCS 0.83 0.60 0.72 0.84 0.72 0.60 0.42 0.18 0.53 - - -\\nFSD 0.93 0.69 0.73 0.91 0.73 0.62 0.65 0.69 0.59 - - -\\nFSD-vec 0.93 0.64 0.73 0.85 0.69 0.61 0.64 0.58 0.56 - - -\\nLLaMA\\np=0.95 0.94 0.94 0.75 0.90 0.93 0.64 - - - - - -\\nCS 0.90 0.78 0.73 0.90 0.73 0.61 - - - - - -\\nFSD 0.93 0.88 0.75 0.93 0.85 0.65 - - - - - -\\nFSD-vec 0.92 0.94 0.74 0.91 0.88 0.64 - - - - - -\\nTable 5: Automatic evaluation results on four non-English datasets and three LMs.\\nBLOOM OPT\\nR-1 R-2 R-L R-1 R-2 R-L\\nbeam (size=8) 32.0 6.7 27.8 35.8 5.9 31.2\\np=0.95 27.5 2.9 23.6 24.1 2.5 20.7\\nCS 34.1 5.5 30.4 35.6 8.3 31.3\\nFSD 34.2 5.9 31.3 37.4 9.8 33.7\\nFSD-vec 33.2 5.2 29.1 37.1 8.6 32.1\\nTable 6: Automatic evaluation results on XSum.\\nindicating FSD/FSD-vec can be a universal choice\\nfor open-ended text generation.\\nInstruction Following The latest generation of\\nLLMs such as ChatGPT (OpenAI, 2022) and\\nLLaMA-2-chat (Touvron et al., 2023b) have the ca-\\npabilities to perform various tasks by following natu-\\nral language instructions. This instruction-following\\napproach has become the standard form for har-\\nnessing LLMs. Therefore, we compare FSD/FSD-\\nvec against baselines within this context. Specif-\\nically, we follow the widely accepted comparison\\nmethod top-p CD CS FSD FSD-vec\\nwin rate 77.20 - 78.32 82.32 81.84\\nTable 7: Win rate of GPT -4 evaluation. CD is omit-\\nted since it requires a smaller amateur model and\\nthe model we use is already the smallest one.\\nsetting (Li et al., 2023b), i.e., reporting the win\\nrates against text-davinci-003 on the alpaca_eval\\ndataset6 with the help of GPT -4 (OpenAI, 2023).\\nWe adopt the LLaMA-2-7b-chat model (Touvron\\net al., 2023b) since it is among the most popu-\\nlar instruction-tuned models. The results of dif-\\nferent decoding methods are shown in Table 7.\\nThe results clearly indicate that FSD/FSD-vec out-\\nperforms the baselines, thus further validating the\\neffectiveness of our approach.\\n6https://huggingface.co/datasets/tatsu-lab/\\nalpaca_eval'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='Close-Ended Generation Task: Summarization\\nSo far, our evaluation has been focused on open-\\nended text generation and general-purpose instruc-\\ntion following. We also evaluate our methods on\\na specific, close-ended generation task: summa-\\nrization. We use the XSum dataset (Narayan et al.,\\n2018). As shown in Table 6, FSD/FSD-vec is gen-\\nerally better than other baselines. It showcases\\nthat our methods can also work well in close-ended\\nscenarios.\\n5.5. Hyperparameter Analysis\\nAnalysis of α We first study the effect of the\\npenalty strength α. We present the results in Ta-\\nble 8. We notice that as α increases, the div score\\nconsistently increases. This is an expected out-\\ncome, as a larger α imposes a greater penalty\\non repetitive content, thereby promoting increased\\ndiversity in the model’s outputs. The coh score\\ndemonstrates a decreasing trend. The reason is\\nthat penalizing the most probable tokens may dam-\\nage the coherence between the prefix and the con-\\ntinuation. Consequently, we see that the mauve\\nscore initially shows an upward trend and then\\nexperiences a slight decrease.\\nAnalysis ofn We study the effect of the hyper-\\nparameter n as shown in Table 9. We can observe\\nthat diversity and coherence are very stable for\\ndifferent n, when n >3, the mauve begins to de-\\ncrease.\\nAnalysis of k We investigate the impact of\\nthe hyperparameter k, as presented in Table 10.\\nWhen k is assigned a minimal value, a notably\\nlower diversity (div) is observed. This can be at-\\ntributed to the reduced search space associated\\nwith a smaller k, which consequently constrains\\nthe diversity of generated outcomes. Conversely,\\nupon incrementing k past a specific threshold, all\\nevaluated metrics—diversity, mauve, and coher-\\nence—demonstrate substantial stability, with only\\nnegligible fluctuations observed. This stability sug-\\ngests that the effective selection space of FSD\\npredominantly comprises a limited number of top\\ntokens.\\nDespite that the hyperparameters can take dif-\\nferent values, we recommend using the default set-\\ntings of those hyperparameters and only adjusting\\nα to suit different tasks.\\n6. Conclusion and Future Directions\\nWe proposed FSD, an effective, efficient, and uni-\\nversal decoding method for avoiding the degen-\\neration problem and improving generation quality.\\nFSD constructs an anti-LM on-the-fly to penalize\\ndiv mau coh\\nα = 1 0.62 0.88 0.67\\nα = 2 0.87 0.94 0.66\\nα = 3 0.93 0.93 0.66\\nα = 4 0.95 0.89 0.65\\nTable 8: Analysis of α. The experiments are con-\\nducted on wikinews using GPT2-XL with FSD.\\ndiv mauve coh\\nn = 2 0.93 0.93 0.64\\nn = 3 0.93 0.94 0.65\\nn = 4 0.92 0.88 0.65\\nTable 9: Analysis of n. The experiments are con-\\nducted on wikinews using GPT2-XL with FSD.\\ndiv mauve coh\\nk = 2 0.69 0.90 0.66\\nk = 4 0.91 0.92 0.65\\nk = 6 0.93 0.93 0.64\\nk = 8 0.94 0.94 0.64\\nk = 10 0.94 0.94 0.64\\nTable 10: Analysis of k. The experiments are con-\\nducted on wikinews using GPT2-XL with FSD.\\nrepetitive generation. Extensive evaluations and\\nanalyses confirm its effectiveness across open-\\nended text generation, instruction following, and\\nsummarization tasks. In addition, FSD demon-\\nstrates better efficiency and generality compared\\nwith existing state-of-the-art decoding methods.\\nAn intriguing future research direction could in-\\nvolve a more nuanced approach to repetitions. In\\nfact, some grams (like named entities) might not\\nrequire penalization at all. Therefore, researchers\\nmay develop more meticulous algorithms based\\non FSD to discern the contexts and conditions un-\\nder which repetitions should be penalized. This\\nwould enable a more refined and context-sensitive\\napplication of repetition management in text gener-\\nation.\\nEthics Statement\\nDue to the nature of language models, we note that\\nthe generations of our method may have offensive,\\ntoxic, unfair, or unethical content. The generations\\nof our method may also have hallucinated content\\nand can be misleading. When deployed in real-\\nworld applications, special attention should be paid\\nto avoid inappropriate generations. For example,\\none can use post-process steps such as toxicity\\nidentification and fact checking.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='7. References\\nBigScience. 2023. Bloom: A 176b-parameter open-\\naccess multilingual language model.\\nTom B. Brown, Benjamin Mann, Nick Ryder,\\nMelanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish\\nSastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-Voss, Gretchen Krueger, Tom Henighan,\\nRewon Child, Aditya Ramesh, Daniel M. Ziegler,\\nJeffrey Wu, Clemens Winter, Christopher Hesse,\\nMark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher\\nBerner, Sam McCandlish, Alec Radford, Ilya\\nSutskever, and Dario Amodei. 2020. Language\\nmodels are few-shot learners. In Advances in\\nNeural Information Processing Systems 33: An-\\nnual Conference on Neural Information Process-\\ning Systems 2020, NeurIPS 2020, December\\n6-12, 2020, virtual.\\nKawin Ethayarajh. 2019. How contextual are con-\\ntextualized word representations? Comparing\\nthe geometry of BERT, ELMo, and GPT-2 em-\\nbeddings. In Proceedings of the 2019 Con-\\nference on Empirical Methods in Natural Lan-\\nguage Processing and the 9th International Joint\\nConference on Natural Language Processing\\n(EMNLP-IJCNLP), pages 55–65, Hong Kong,\\nChina. Association for Computational Linguis-\\ntics.\\nAngela Fan, Mike Lewis, and Y ann Dauphin. 2018.\\nHierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Associ-\\nation for Computational Linguistics (Volume 1:\\nLong Papers).\\nZihao Fu, Wai Lam, Anthony Man-Cho So, and Bei\\nShi. 2021. A theoretical analysis of the repetition\\nproblem in text generation. In Thirty-Fifth AAAI\\nConference on Artificial Intelligence, AAAI 2021,\\nThirty-Third Conference on Innovative Applica-\\ntions of Artificial Intelligence, IAAI 2021, The\\nEleventh Symposium on Educational Advances\\nin Artificial Intelligence, EAAI 2021, Virtual Event,\\nFebruary 2-9, 2021.\\nTianyu Gao, Xingcheng Y ao, and Danqi Chen.\\n2021. SimCSE: Simple contrastive learning of\\nsentence embeddings. In Proceedings of the\\n2021 Conference on Empirical Methods in Natu-\\nral Language Processing.\\nJohn Hewitt, Christopher Manning, and Percy\\nLiang. 2022. Truncation sampling as language\\nmodel desmoothing. In Findings of the Asso-\\nciation for Computational Linguistics: EMNLP\\n2022.\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes,\\nand Y ejin Choi. 2020. The curious case of neural\\ntext degeneration. In 8th International Confer-\\nence on Learning Representations, ICLR 2020,\\nAddis Ababa, Ethiopia, April 26-30, 2020.\\nDan Jurafsky and James H. Martin. 2009. Speech\\nand language processing : an introduction to\\nnatural language processing, computational lin-\\nguistics, and speech recognition.\\nEvgeny Lagutin, Daniil Gavrilov, and Pavel Kalaidin.\\n2021. Implicit unlikelihood training: Improving\\nneural text generation with reinforcement learn-\\ning. In Proceedings of the 16th Conference of the\\nEuropean Chapter of the Association for Com-\\nputational Linguistics: Main Volume.\\nTian Lan, Yixuan Su, Shuhang Liu, Heyan Huang,\\nand Xian-Ling Mao. 2022. Momentum decod-\\ning: Open-ended text generation as graph explo-\\nration.\\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy\\nLiang, Jason Eisner, Tatsunori Hashimoto, Luke\\nZettlemoyer, and Mike Lewis. 2023a. Contrastive\\ndecoding: Open-ended text generation as opti-\\nmization. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers).\\nXuechen Li, Tianyi Zhang, Y ann Dubois, Ro-\\nhan Taori, Ishaan Gulrajani, Carlos Guestrin,\\nPercy Liang, and Tatsunori B. Hashimoto.\\n2023b. Alpacaeval: An automatic evaluator of\\ninstruction-following models. https://github.\\ncom/tatsu-lab/alpaca_eval.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao\\nJiang, Hiroaki Hayashi, and Graham Neubig.\\n2022. Pre-train, prompt, and predict: A sys-\\ntematic survey of prompting methods in natural\\nlanguage processing. ACM Comput. Surv. Just\\nAccepted.\\nClara Meister, Tiago Pimentel, Gian Wiher, and\\nRyan Cotterell. 2022. Typical decoding for\\nnatural language generation. ArXiv preprint ,\\nabs/2202.00666.\\nStephen Merity, Caiming Xiong, James Bradbury,\\nand Richard Socher. 2017. Pointer sentinel mix-\\nture models. In 5th International Conference on\\nLearning Representations, ICLR 2017, Toulon,\\nFrance, April 24-26, 2017, Conference Track Pro-\\nceedings.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Shashi Narayan, Shay B. Cohen, and Mirella Lap-\\nata. 2018. Don’t give me the details, just the sum-\\nmary! topic-aware convolutional neural networks\\nfor extreme summarization. In Proceedings of\\nthe 2018 Conference on Empirical Methods in\\nNatural Language Processing.\\nOpenAI. 2022. Introducing chatgpt. https://\\nopenai.com/blog/chatgpt.\\nOpenAI. 2023. GPT -4 technical report. ArXiv\\npreprint, abs/2303.08774.\\nKrishna Pillutla, Swabha Swayamdipta, Rowan\\nZellers, John Thickstun, Sean Welleck, Y ejin\\nChoi, and Zaïd Harchaoui. 2021. MAUVE: mea-\\nsuring the gap between neural text and human\\ntext using divergence frontiers. In Advances in\\nNeural Information Processing Systems 34: An-\\nnual Conference on Neural Information Process-\\ning Systems 2021, NeurIPS 2021, December\\n6-14, 2021, virtual.\\nAlec Radford, Jeffrey Wu, Rewon Child, David\\nLuan, Dario Amodei, Ilya Sutskever, et al. 2019.\\nLanguage models are unsupervised multitask\\nlearners. OpenAI blog, 1(8).\\nAbigail See, Peter J. Liu, and Christopher D. Man-\\nning. 2017. Get to the point: Summarization\\nwith pointer-generator networks. In Proceedings\\nof the 55th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long\\nPapers).\\nChufan Shi, Haoran Y ang, Deng Cai, Zhisong\\nZhang, Yifan Wang, Yujiu Y ang, and Wai Lam.\\n2024. A thorough examination of decoding meth-\\nods in the era of llms.\\nShuming Shi, Enbo Zhao, Duyu Tang, Y an Wang,\\nPiji Li, Wei Bi, Haiyun Jiang, Guoping Huang,\\nLeyang Cui, Xinting Huang, et al. 2022. Ef-\\nfidit: Y our ai writing assistant. ArXiv preprint,\\nabs/2208.01815.\\nYixuan Su, Tian Lan, Y an Wang, Dani Y ogatama,\\nLingpeng Kong, and Nigel Collier. 2022. A con-\\ntrastive framework for neural text generation. In\\nAdvances in Neural Information Processing Sys-\\ntems.\\nYixuan Su and Jialu Xu. 2022. An empirical study\\non contrastive search and contrastive decoding\\nfor open-ended text generation.\\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le.\\n2014. Sequence to sequence learning with neu-\\nral networks. In Advances in Neural Informa-\\ntion Processing Systems 27: Annual Confer-\\nence on Neural Information Processing Systems\\n2014, December 8-13 2014, Montreal, Quebec,\\nCanada.\\nPaolo Tonella, Roberto Tiella, and Cu Duy Nguyen.\\n2014. Interpolated n-grams for model based test-\\ning. In Proceedings of the 36th International Con-\\nference on Software Engineering, ICSE 2014.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard,\\nXavier Martinet, Marie-Anne Lachaux, Timothée\\nLacroix, Baptiste Rozière, Naman Goyal, Eric\\nHambro, Faisal Azhar, Aurelien Rodriguez, Ar-\\nmand Joulin, Edouard Grave, and Guillaume\\nLample. 2023a. Llama: Open and efficient foun-\\ndation language models.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter\\nAlbert, Amjad Almahairi, Y asmine Babaei, Niko-\\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cris-\\ntian Canton Ferrer, Moya Chen, Guillem Cucu-\\nrull, David Esiobu, Jude Fernandes, Jeremy Fu,\\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\\nGoswami, Naman Goyal, Anthony Hartshorn,\\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin\\nKardas, Viktor Kerkez, Madian Khabsa, Isabel\\nKloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier\\nMartinet, Todor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Yixin Nie, Andrew Poulton, Jeremy\\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan\\nSchelten, Ruan Silva, Eric Michael Smith, Ran-\\njan Subramanian, Xiaoqing Ellen Tan, Binh Tang,\\nRoss Taylor, Adina Williams, Jian Xiang Kuan,\\nPuxin Xu, Zheng Y an, Iliyan Zarov, Yuchen\\nZhang, Angela Fan, Melanie Kambadur, Sha-\\nran Narang, Aurelien Rodriguez, Robert Stojnic,\\nSergey Edunov, and Thomas Scialom. 2023b.\\nLlama 2: Open foundation and fine-tuned chat\\nmodels.\\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily\\nDinan, Kyunghyun Cho, and Jason Weston.\\n2020. Neural text generation with unlikelihood\\ntraining. In 8th International Conference on\\nLearning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020.\\nGian Wiher, Clara Meister, and Ryan Cotterell.\\n2022. On decoding strategies for neural text\\ngenerators. Transactions of the Association for\\nComputational Linguistics, 10:997–1012.\\nJin Xu, Xiaojiang Liu, Jianhao Y an, Deng Cai,\\nHuayang Li, and Jian Li. 2022. Learning to\\nbreak the loop: Analyzing and mitigating rep-\\netitions for neural text generation. ArXiv preprint,\\nabs/2206.02369.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\\nArtetxe, Moya Chen, Shuohui Chen, Christopher\\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor\\nMihaylov, Myle Ott, Sam Shleifer, Kurt Shuster,'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='Daniel Simig, Punit Singh Koura, Anjali Sridhar,\\nTianlu Wang, and Luke Zettlemoyer. 2022. Opt:\\nOpen pre-trained transformer language models.\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan\\nSalakhutdinov, Raquel Urtasun, Antonio Tor-\\nralba, and Sanja Fidler. 2015. Aligning books\\nand movies: Towards story-like visual explana-\\ntions by watching movies and reading books. In\\n2015 IEEE International Conference on Com-\\nputer Vision (ICCV), pages 19–27.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Algorithm 2:FSD Decoding\\nInput : the LM pθ (e.g. GPT2); the anti-LM pω;\\nthe prompt text x1:l; the decoding length\\nm; the stopwords set S; the punctuation\\nset P;\\n1 Construct the anti-LM pω with the prompt x1:l;\\n2 for step t = l + 1to l + m do\\n3 Compute next token distribution pθ(·|x<t);\\n4 Get V (k) from pθ(·|x<t);\\n5 for candidate v ∈ V (k) do\\n6 Get the penalty pω(v|x<t) according to\\nEq. 3 (discrete version) or Eq. 4\\n(vectorized version);\\n7 xt = arg maxv∈V (k) {FSD(v|x<t)};\\n8 Update pω with xt;\\nOutput :The generated text ˆx.\\nA. Implementation Details\\nWe provide the detailed pseudo code of FSD in\\nAlg. 2.\\nStopwords and Punctuations Stopwords signifi-\\ncantly influence the diversity of sentence structures\\nas they often appear at the beginning of sentences,\\nsuch as \"The...\" or \"He...\". To provide finer control\\nover the penalty applied to stopwords, we introduce\\na discount factor ϕ. This factor is multiplied by the\\nsecond term of Eq. 2, replacing α with ϕ · α specifi-\\ncally for stopwords. A smaller ϕ tends to produce\\nsentences with similar structures, as demonstrated\\nin the example provided in Table 13. Conversely,\\na larger ϕ can lead to the generation of invalid\\nsentences due to the heavy penalty imposed on\\nstopwords at the beginning of a sentence. This\\nmay result in the selection of incorrect tokens, as\\nillustrated in the example presented in Table 14.\\nWe also experimentally find that penalizing punc-\\ntuations can sometimes introduce grammar errors\\nin the generated text. Specifically, when utilizing\\nGPT2 as the base model, we have found that the\\npunctuation symbols ˙C˙C (representing \"\\\\n \\\\n\") and\\n˙C (representing \"\\\\n\") have a significant impact on\\nthe grammatical correctness of the output. An\\nexample illustrating this phenomenon is provided\\nin Table 15. In our experiments, we do not pe-\\nnalize punctuations and the punctuations set is\\nP =\\n\\x08\\n. , : \" ‘ ˙C˙C ˙C\\n\\t\\n.\\nHyper-parameter Settings We search ϕ from\\n{0.2, 0.4, 0.6, 0.8, 1, 1.5}. The detailed parameter\\nsettings are listed in Table 11.\\nn α ϕ\\nFSD\\nwikinws 3 3 0.2\\nwikitext 3 3 0.4\\nbook 3 3 0.6\\nFSD-vec\\nwikinws 2 1 0.2\\nwikitext 2 1 0.2\\nbook 2 1 0.6\\nTable 11: Parameter settings of ϕs\\nB. Further Analysis\\nB.1. Effect of Smoothing\\nIn previous experiments, we adopt the smoothed\\nn-gram model as the anti-LM. To understand the\\neffect of smoothing, we also implement the un-\\nsmoothed n-gram and run multiple experiments\\nwith different n ∈ [1, 2, 3, 4]. Then, we calculate\\nREP-i, i∈ [2, 3, 4]. The results are illustrated in\\nFigure 4. We found that if unsmoothed n-gram is\\napplied, the best performance is achieved when\\nn = 2. The reason for this phenomenon is that\\nif n >2, the unsmoothed n-gram LM can not pe-\\nnalize grams with lengths smaller than n, which\\nis manifested by the high REP-i, i < nin Figure 4.\\nHowever, setting n = 2sometimes may not be a\\ngood option due to the BPE encoding algorithm,\\nunder which a word (e.g., name of a person) can\\nbe decomposed into multiple tokens. If penalized\\nheavily, these words may not be recovered.\\n/uni00000015 /uni00000016 /uni00000017\\n/uni00000035/uni00000028/uni00000033/uni00000010/uni0000004c\\n/uni00000013\\n/uni00000015/uni00000013\\n/uni00000017/uni00000013\\n/uni00000019/uni00000018/uni00000055/uni00000048/uni00000053/uni00000011/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048\\n/uni00000051/uni00000020/uni00000014\\n/uni00000051/uni00000020/uni00000015\\n/uni00000051/uni00000020/uni00000016\\n/uni00000051/uni00000020/uni00000017\\nFigure 4: Repetition rate for different n.\\nC. More Cases\\nC.1. FSD vs CS\\nWe provide more cases on Table 16, Table 17, Ta-\\nble 18 for wikinews, wikitext and book respectively\\nto compare FSD with CS.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='C.2. FSD vs FSD-vec\\nWe provide more cases on Table 19, Table 20, Ta-\\nble 21 for wikinews, wikitext and book respectively\\nto compare FSD with FSD-vec.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content='Prompt: Following months of rumours, musician Wyclef Jean today confirmed that he is considering standing for\\nthe presidency of Haiti in the country’s November elections. A statement from\\nCS: Jean’s publicist reads: \"Wyclef will make a decision on his political future in the coming days and is consulting\\nwith his family and advisors. \"The announcement comes after Jean, who has been touring the world in support of\\nhis latest album A New Day In The Old Town, was forced to cancel a concert in the Dominican Republic last week\\ndue to security concerns. He had been due to play the concert in Santo Domingo on November 4 but the concert\\nwas called off after Dominican authorities raised concerns about the safety of the concert venue, which is close to\\nthe border with Haiti. In an interview with Rolling Stone, Jean said he was \"saddened\" by the cancellation, but\\nthat he had no choice but to cancel the concert. \"I’m a human being and it’s my job to do what I have to do,\" he\\nsaid. \"If there’s a risk, I have to take it. But I can’t go to the Dominican Republic with this threat hanging over my\\nhead.\"Jean’s tour, which has taken him to South Africa, the United Arab Emirates (UAE, where he performed at\\nthe Abu Dhabi Formula One Grand Prix in October) and Brazil, is the first time he has performed in Haiti since\\nFSD: Jean’s management company, La Nuit Productions, said: \"Wyclef is currently in the process of gathering\\ninformation and will make a decision in the coming days.\"Jean, who has been a vocal supporter of the Haitian\\ngovernment, has previously said he would consider running for the position. The musician, who is also a member\\nof the hip-hop group A Tribe Called Quest, has not yet announced his candidacy. Haiti’s current president, Michel\\nMartelly, is widely expected to run for re-election. In a statement, Jean said: Ï am considering running for president\\nof Haiti. I have been following the election process closely and I believe that Haiti needs a new leader who\\ncan bring about change and rebuild the country. \"I have always been a supporter of President Martello and his\\ngovernment. I am also a fan of the music of A Tribe called Quest and have supported them in the past. \"The\\nHaitian people have suffered greatly under the current government and I feel that the time has come for a change.\\n\"Hearing the news of the earthquake in Haiti, I was deeply saddened and saddened to hear of the loss of life and\\nthe destruction of homes and businesses.\\nTable 12: One case of FSD versus CS. The prefix’s topic is “a musician is condsiering runing for\\npresidency”. But the topic of CS’s output is concert tours which is irrelevant to that of the prefix. By\\ncontrast, FSD always sticks to the topic of the prefix.\\nPrompt: A gas explosion in the Grand Riviera Princess resort in Playa del Carmen, Mexico earlier today killed\\nseven people and left 20 people injured, two critically according to\\nGenerations: local media. The explosion occurred at around 8:30am local time (10:00am GMT) at the hotel’s\\npool area, which is located on the second floor of the resort.The hotel is a popular destination for tourists and\\nis known for its large pool and spa. The cause of the explosion is still unknown, but local authorities have said\\nthat the cause is not related to the fire that broke out in the pool earlier in the day. The resort’s management has\\nsaid that the incident is not connected to the blaze that erupted in the same area earlier in the day.The fire was\\nextinguished by the resort’s staff and the guests were evacuated. The victims of the blast have been identified\\nas: A man in his 30s A woman in her 20s A young man in his 20s A middle-aged woman A 40-year-old man A\\n50-year old woman A 60-something woman A 70-some-year women A man in his 50s A woman in her 30s A\\n30-something man A 20-something girl A 15-some year\\nPrompt: According to data from South Korea’s national bank, the country’s economy grew by 0.2% in the last\\nquarter of 2009. The rise is now\\nGenerations: expected to be 0.3% in the first quarter of 2010. The government has been trying to boost the\\neconomy by cutting taxes and increasing spending.The central bank has also been cutting interest rates to\\nstimulate the economy.The Bank of Korea has cut its benchmark interest rate to 1.5% from 2.5%.The rate is still\\nhigher than the rate of inflation.The country has been struggling with a huge debt burden.The debt is estimated at\\nabout $1.4 trillion.The South Korean government has been attempting to reduce the debt by selling off state-owned\\ncompanies.The government has been selling off the Hyundai Motor Group, the nation’s largest carmaker, and the\\nKorea Development Bank.The state has also been selling off its stake in the Samsung Group.The sale of the two\\ncompanies has been a success.The Hyundai Group has been sold for $8.6 billion.The Korea DevelopmentBank\\nhas been sold for about half that amount.The two sales have been a big success.The sales have helped the\\ngovernment to lower its debt to GDP ratio.The debt to the GDP is now down to about 50% of GDP .The Korean\\neconomy is expected to grow by 1.\\nTable 13: No penalty on stopwords may procude sentences with similar grammatical structure.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content='Prompt:Charlie, a Chimpanzee in a South African zoo who became addicted to cigarettes, has died at the age of\\n52, exceeding the average lifespan of a captive\\nGeneration: animal. The cause of death was not immediately known. Charlie, who was born in captivity in 1989,\\nwas diagnosed with lung cancer in 2011. He was given a lung transplant in 2013, but his health deteriorated again\\nand he was put down. His death comes just weeks after another chimpanzee, Harambe, was shot dead by zoo\\nofficials after a child fell into his enclosure. Image copyright Getty Images Image caption Charlie was one of the\\nlast chimps to be born at an American zoo In a statement, the zoo said: \"Charlie’s passing is a great loss to the\\nanimal kingdom. \"Charlie was a beloved member of our family and we will miss him dearly.\" Chimpanzees are the\\nclosest living relatives of humans, and are often used in research. They are also used for entertainment, as they\\nare highly intelligent and can mimic human speech. Charlie, whose name means \"chimney sweep\" in Swahili,\\nwas born on 19 April 1989 at San Diego Zoo. He lived at the San Francisco Zoo for five years before moving to\\nCincinnati Zoo in July 2014. Image caption Chimps are known for their intelligence and mimicry The\\nPrompt:Police have said that an explosion in a hotel garage in San Pedro, Los Angeles, California, that sparked\\nan evacuation of a large area was an act of vandalism\\nGeneration: . The incident occurred at around 2:30am local time (11:00 GMT) on Thursday. A fire broke out in\\nthe garage, which is located near the hotel, and caused a huge explosion. Image copyright Getty Images Image\\ncaption The hotel was evacuated after the explosion \"It was a very loud explosion,\" said one witness. \"I was\\nsleeping and woke up to the sound of the explosion. I looked out the window and saw a lot of smoke and fire.\"\\nAnother witness, who did not want to be named, said: \"I heard a loud bang and then I saw the fire. \"The fire was\\nso big it was like a forest fire. It was just like something out of a movie.\" Media playback is unsupported on your\\ndevice Media caption Footage shows the aftermath of the blast Los Angeles Mayor Eric Garcetti said the incident\\nwas \"an act against our city\". \"We will not tolerate this type of violence,\" he said. \"This is an attack on our way of\\nlife, our economy, our infrastructure, and our future.\" He said he had ordered the evacuation \"out of an abundance\\nof caution\". \"Our city\\nTable 14: Penalizing stopwords heavily may produce invalid generations. The invalid generation “Image\\ncopyright ...” may occur in some examples because the stopwords that have a larger probability than\\n“Image” have a larger penalty. Therefore, “Image” is selected.\\nPrompt: Apple Inc. announced Friday that it will give owners of its new iPhone 4 a free case in response to\\nmounting concerns over the device’s antenna placement. The iPhone\\nGeneration: maker said it is offering the free cases to customers who have already purchased the phone. The\\ncompany said the new cases will be available in stores and online starting Friday. Apple said the cases will have a\\n\"unique design\" that will help protect the iPhone’s antennas from damage. , which is made by Corning Inc., will be\\nsold in the U.S. and Canada for $29.99. The company also said that the case will be made in China, where the\\ncompany has been making its products for years. \"We are committed to providing our customers with the best\\npossible products and we are always looking for ways to improve our products and services,\" Apple spokeswoman\\nKristin Huguet said in a statement. \"We know that our fans want to protect their iPhone and this case is designed\\nto do just that.\" Apple’s decision to offer the free protective cases comes after a number of reports of people\\nexperiencing problems with their iPhones’ antennas. Some people have said they have been unable to make calls\\nor send text messages because their phones were unable to receive signals. Others have complained that their\\ndevices have lost reception or have been damaged by the antenna. a report by The Wall Street Journal last week\\nthat said some iPhone owners had been complaining\\nPrompt:Wikinews reporter Iain Macdonald has performed an interview with Dr Isabella Margara, a London-based\\nmember of the Communist Party of Greece (K\\nGeneration: KE), who has been a member of the party since the 1970s. Dr Margaras is a well-known figure in the\\nGreek left, having been a prominent member of the KKE since the 1980s, and having served as a deputy to the\\nparty’s leader, Dimitris Koutsoumbas. She is also a former member of the Socialist Party (PASOK) and a long-time\\nmember of the European Parliament. In the interview, she discusses the current situation in Greece, the role of\\nthe EU in the country’s crisis, and the possibility of a new left government in Athens. and the role of the EU in the\\ncountrys crisis,. In the article, Dr Margareta Margara also discusses the recent decision by the Greek government\\nto impose capital controls on the banks, and the implications of this for the country. The interview was conducted\\nby Iain MacDonald on behalf of The Independent. Iain: What is the situation in your country? Margaras: The\\nsituation is very bad. The government has imposed capital control on the banking system. This is a very serious\\nsituation. The banks are closed and there is no access to money. There is no money in the banks. There is\\nTable 15: Penalizing punctuations can produce grammar errors.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='Prompt-1: On Friday, the 189 Parties to the Nuclear Non-Proliferation Treaty (NPT) agreed to a final resolution\\ncalling for steps toward the banning of weapons\\nFSD: of mass destruction. The resolution, which was adopted by consensus, calls for the \"immediate and\\ncomplete cessation of all nuclear, chemical and biological weapons-related activities\" and \"the establishment of\\na comprehensive, verifiable and irreversible nuclear-weapons-free zone in the Middle East.\" It also calls for \"a\\ncomprehensive, effective and enforceable prohibition on the production, stockpiling, acquisition, transfer, use and\\nthreat of use of nuclear weapons and all other weapons of indiscriminate and disproportionate impact.\" The NPT\\nwas created in 1968 to prevent the spread of nuclear and other arms. \"The international community has been\\nworking for many years to achieve a world without nuclear weapons,\" said U.S. Ambassador to the United Nations\\nSamantha Power. \"Today, we have taken a major step forward toward that goal.\" \"This is a historic day for the\\ninternational non-proliferating movement,\" she said. \"It is a day of hope and optimism for all those who believe\\nthat the world can be a safer, more peaceful place.\" \"We are proud of the work that has been done by the 193\\nN.G.O. member states to reach this historic agreement,\" Power said. \"We will continue to\\nCS: of mass destruction. In the meantime, the United States is continuing to develop nuclear weapons and\\nballistic missile defense systems that it can use against Iran if it were to violate the terms of the agreement. On\\nMonday, Secretary of State John Kerry said Iran’s \"threats and provocations are a clear violation of the spirit and\\nletter\" of the NPT, which was signed by 122 countries in the 1970s. Kerry’s comments came during a visit to\\nSouth Korea, where he met with President Park Geun-hye and Foreign Minister Yun Byung-se. \"We have made it\\nclear to the Iranians that their nuclear program must be subject to IAEA safeguards, and that they must cease\\nwork on a heavy water reactor that could produce plutonium,\" Kerry said. The Obama administration has been\\nworking for more than a year to reach a deal with Iran that would curb its nuclear program in exchange for relief\\nfrom economic sanctions. But negotiations have hit snags, most recently over Iran’s refusal to allow inspectors\\ninto Parchin, a military site where Iran is believed to have carried out experiments on nuclear weapons in the past.\\nIranian officials have said the site is for research and development, but Western intelligence\\nPrompt-2: Inventor Walter Frederick Morrison has died of the age of 90. He was famous for being the inventor of\\nthe flying disc, more commonly identified as the fr\\nFSD: isbee. Morrison was born in New Y ork City in 1891. His father was a lawyer and his mother was a\\nhomemaker. Morrison’s father died when he was young, and he moved to Chicago to live with his aunt. He\\nattended the University of Chicago, where he studied engineering. In 1915, he joined the U.S. Army Air Corps,\\nand was stationed in France during World War I. In 1919, Morrison was commissioned as an officer in the Army\\nSignal Corps. He served in the Signal Service, which was responsible for transmitting messages across the\\nAtlantic. After the war, Morrison went to work for the Bell Telephone Company, where his job was to design and\\nbuild telephone systems. In 1928, he founded the Morrison Electric Company, which manufactured and sold\\nelectric motors and generators. In 1939, Morrison founded the American Aircraft Corporation, which made aircraft\\nengines. In 1941, the company was sold to the General Electric Corporation, and Morrison became president of\\nthe new company. The company’s first aircraft, the P-51 Mustang, was designed by Morrison. The P2V Mustang,\\na fighter plane, was the first American fighter to be powered by a jet engine.\\nCS: sbee. Morrison’s name is on the back of every disc sold, and the discs are made in his hometown of St. Louis,\\nMissouri. The disc was invented in 1887 by a man named George Eastman, who patented it in the United States\\nand sold it to Parker Brothers for $1,500. It was a flimsy disc with a hole in the middle that could be thrown with\\none hand, according to Smithsonian Magazine. In 1889, Parker Brothers began to sell the disc in Europe, and by\\nthe 1920s, there were over 2 million discs in circulation, according to Smithsonian Magazine. But the disc was not\\nwithout its problems. It was difficult to make, and many people complained about the disc breaking on the way\\nto the basket. \"I have seen it, and it is terrible,\" one golfer said, according to Smithsonian Magazine. \"The ball\\ngoes in, and you have to hold it in your hand for a long time, and then it breaks in the basket.\" The frisbee, on the\\nother hand, was much more durable and easy to throw. \"It’s like a golf club in that way,\" Morrison told National\\nGeographic in a 2011 interview. \"\\nTable 16: Comparison of CS and FSD on wikinews. We can see that Both FSD and CS can generate\\nfluent sentences. However, FSD is more sticky to the topic on these two prompts.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content='Prompt-1: <|endoftext|> With a satisfying protagonist completed, Sega turned to esteemed programmer Yuji\\nNaka, who had impressed them through his work on Phantasy Star and the Genesis port\\nFSD: of Sonic the Hedgehog 2. Nana was a new face to the company, but he had a reputation for being a hard\\nworker and a good communicator. Nana’s first task was to create a character that would be able to stand out\\nfrom the crowd. He chose Sonic, a character who was already well known in Japan. The game’s story was also\\ndesigned to be unique, with the player controlling Sonic as he travels through a world that is not only different from\\nany other Sonic game, but also from other games in the Sonic series. The game was originally going to be called\\nSonic Adventure, but Sega decided to change the name to Sonic the Fighters. Sega’s decision to use a different\\nname for the game was not a coincidence. In the early 1990s, the Japanese video game industry was in a state of\\nflux. Sega was struggling to find its place in the market, and it was looking for a way to differentiate itself from\\nNintendo. In the United States, Nintendo had been dominating the market for years, and Sega had struggled to\\ngain a foothold. As a result, Sega began to look for ways to differentiate itself. One of the ways Sega tried to do\\nthis was by creating a game that was\\nCS: of Final Fantasy. Naka was brought in to work on Sonic the Hedgehog, and the result was a game that had\\neverything you could want in a Sonic game: speed, platforming, a sense of humor, and, of course, lots and lots of\\nexplosions. Sonic 3 & Knuckles is an incredibly well-made game, and the story is one of the best in the series to\\nthis day. It’s not perfect, but it’s a great example of what can be done with the Sonic formula. In this article, we’re\\ngoing to take a look at some of the things that make Sonic 3 & Knuckles such a great game, and how they were\\nimplemented in the game. If you’re new to Sonic, this is a great place to start. 1. The Game Over screen is the\\nbest part of the game The game over screen is where you get to see your score, how many lives you have left,\\nand what level you’re on. It’s a great way to end a game, and it’s one of the most memorable parts of the game. 2.\\nThere are a lot of secrets to find The levels are packed with secrets, and they’re hidden all over the\\nPrompt-2: <|endoftext|> \" Kiss Y ou \" was written by Kristoffer Fogelmark, Kristian Lundin, Albin Nedler, Savan\\nKotecha, Shellback\\nFSD: , and Jens-Christian Svenning. It was produced by the Swedish Film Institute and directed by Jonas\\nÅkerlund. The film was released in the United States on October 21, 2010. Plot [ edit ] A young man, who is a\\nmember of the \"Kiss Y ou\" group, is on a date with a woman. He is very nervous and tries to hide his feelings. The\\nwoman, however, is not interested in him and leaves. The man is left alone and begins to cry. Later, the man\\nmeets a man who has been in a coma for a year. The two men are friends and the man tells the young guy that\\nhe has a girlfriend. The young boy is shocked and asks the girl if she is his girlfriend. She says that she is not and\\nthat they are just friends. The man then asks if he can kiss her. The girl says no and tells him that it is too soon.\\nThe man says that if it’s not too late, he will kiss her. The young couple then kiss and the woman says \"I love you\".\\nThe film ends with the two smiling. Cast [ Edit ] Production [ edit ]; Credits [ add\\nCS: . Recorded and Produced at Soundworks in Gothenburg, Sweden. Mastered at Sterling Sound in Los\\nAngeles, CA. Additional Recording at Studio G-A-N in Gothenburg. Barcode: 7 27361 85928 2 Matrix / Runout\\n(Variant 1): SON-A-1-SOUNDWAVES-BRAZIL-01-GOTHLAND-AUGUST -2014 (WEA) MADE IN JAPAN BY\\nAPPLE RECORDS A/S. Matrix / Runout (Variant 2): SON-A-1-SOUNDWAVES-BRAZIL-01-GOTHLAND-AUGUST -\\n2014 (WEA) MADE IN JAPAN BY APPLE RECORDS A/S. Mastering SID Code (Variant 1): IFPI L574 Mastering\\nSID Code (Variant 2): IFPI L574 Mould SID Code (Variant 1): IFPI 9085 Mould SID Code (Variant 2): IFPI 9085\\nRights Society: GEMA\\nTable 17: Comparison of CS and FSD on wikitext. CS obviously has a topic drift on Prompt-2.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='Prompt-1: The Sons vowed to combine their wisdoms and forsake their names and worship wholeheartedly the\\nUnmarked Ones so that they one day might return. They would\\nFSD: be the first to bring the world back to the way it was before the fall. The Firstborn In the years that followed,\\nthe Sons of Svanir grew in power and influence. The First Born were the most powerful of their number, and they\\nwere known as the \"Firstborn.\" They were also the most secretive, for they had no need of a public face. Their\\ntrue name was kept secret, but they knew it well. They were born of the same blood as their brothers, but their\\nbodies were different. Instead of being born with a single eye, they had two. This was because the Firstborn\\nwere not born from the same mother as their brethren, but from a different mother. This was not a problem for\\nthe FirstBorn, for their mother was a woman named Nithra. She was the daughter of an Unmarked One named\\nKord. Kords was an old man who had lived for many years. He had been a great warrior, and he had fought in\\nmany wars. One day, Kord was attacked by a group of bandits. He was killed, and his body was taken to a nearby\\nvillage. There, his wife, Nethra, was told of his death\\nCS: be a force of good in the world, and their god-kings would rule with an iron fist. The Sons of Sanguinius had\\nno need for this kind of power, but it was the only way they could keep from being wiped out by the Emperor’s\\nChildren, who were the most powerful Space Marine Legion at the time. In the end, the Emperor’s Children won\\nthe Great Crusade, and Sanguinius was forced to flee into the Eye of Terror, where he remained for a thousand\\nstandard years before being found by the Ultramarines. Siege of Vraks \"Y ou are a fool, Horus. I know what it is to\\nbe an outcast, and the shame is yours.\" — Fulgrim, to Horus during the Siege of Vraks (Unknown Date.M41) In\\n998.M41, the Blood Angels were deployed to the world of Vraks, which had been the site of a bloody civil war\\nbetween the Sons of Sanguinius and the Traitor Legion known as the World Eaters. The Sons of Sanguinius had\\ntaken the planet as a staging ground for their invasion of the Segmentum Obscurus, and were intent on using\\nPrompt-2: But that’s enough about aging, enough gloom and despondency. I shall tell you a story about when I\\nmyself was young and susceptible. It was in\\nFSD: the early days of the war, and I was a student at the University of Chicago. The war was raging, and we were\\nall in a state of excitement. We were not yet aware of the fact that we had been born into a world of death. I was\\nstudying in my room, with my books open on the table. Suddenly I heard a noise outside. I looked out the window,\\nand saw a man running across the street. He was dressed in civilian clothes, but he was carrying a rifle. \"What\\nare you doing?\" I asked. He stopped, looked at me, and then he ran back to his house. The next day, I went to the\\nuniversity, where I found that the man had died. It was a strange experience. For a moment I felt that I had lived in\\na dream. Then I remembered that it was only a dream. But it is a fact, that in those days, when the world was full\\nof fear, there were people who were prepared to die for their country. And it is also a fact, that when we are young,\\nwe have a tendency to forget that there are people in the world who are prepared to sacrifice their lives for us\\nCS: the summer of 1884, and my family was on a visit to New Y ork. We stayed at the Waldorf-Astoria, which was\\nin the heart of Times Square. The hotel was a grand, opulent edifice, with marble floors, gold-trimmed walls, and\\na grand ballroom that could seat two thousand people at a time. I was in the ballroom one day with my mother,\\nfather, and a few of my friends. It was a hot day, and the air was filled with steam from the boilers, which were in\\noperation all over the hotel. We were sitting on the verandah, enjoying the cool breeze and the view of the city\\nbelow. Suddenly there was a commotion in the ballroom, and a man in a black suit rushed in with a pistol in his\\nhand. He pointed it at my mother, and demanded to know who she was. When she answered that she was my\\nmother, the man drew his pistol and shot her point-blank in the chest. She fell to the floor, and I was left to look on\\nin horror. My mother was dead, and there was nothing I could do to save her. The next day, when I got home, I\\nfound a\\nTable 18: Comparison of CS and FSD on book.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Prompt-1: A private security guard, defending the Panamanian based MV Almezaan, killed a Somali pirate who\\nwas attempting to hijack the ship on Tuesday.\\nFSD: The MV Maersk Alabama was sailing from the port of Bremerhaven in Germany to the Gulf of Aden when it\\nwas attacked by a pirate vessel. The crew of the MV Ala’a were able to board the pirate ship and kill the Somali\\npirates. \"The security guards of the Almezan were on board the ship when they were attacked by pirates,\" said a\\nstatement from the German Maritime Ministry. \"The guards were armed with pistols and were able to shoot and\\ninjure the pirates.\" The Almezan was able to return to Bremershaven and continue its journey to the gulf of Aden.\\n\"We are grateful to the security forces of Germany and the United Arab Emirates for their quick response and for\\nthe safe return of the vessel,\" the statement said. \"This is a clear example of the cooperation between the maritime\\nauthorities of the two countries.\" Piracy off the coast of Somalia has been on the rise in recent years. In 2012, the\\nUN Security Council passed a resolution to tackle the problem. The resolution called for a \"comprehensive and\\ncoordinated international response\" to the problem. It also called for the creation of a \"global maritime force\\nFSD-vec: The incident occurred at about 10:30 a.m. local time (1:30 p.m. ET) when the crew of the MV Almezaan,\\na cargo vessel, spotted a small boat in the water off the coast of Somalia. The security guard boarded the vessel\\nand ordered the pirates to surrender. \"The pirate refused to comply with the order and fired a warning shot at the\\nguard,\" said Captain John G. Smith, commander of the U.S. Navy’s 7th Fleet. \"The armed guard then opened fire,\\nkilling the suspected pirate.\" The Almezaan, which is owned by the Dutch company Maersk Line, was carrying\\nabout 1,000 tons of fuel oil, food and other supplies. The Somali Pirates are known for their piracy and have been\\ninvolved in several attacks on ships in the region. In April, a Somali Pirate hijacked a tanker, the MSC Vos Hestia,\\noff the Horn of Africa. The ship was eventually rescued by the Royal Navy. Last year, a group of Somali pirates\\nattacked a merchant shipping vessel, the SSV Punt, off the east coast of Africa. The attack resulted in the\\nPrompt-2: The Space Shuttle Discovery, flying the STS-133 mission, has successfully rendezvoused and docked\\nwith the International Space Station (ISS) today at\\nFSD: 7:30 a.m. EDT. The docking was completed at 8:15 a.m. \"The crew of STS 133 is safely back on Earth,\"\\nsaid NASA Administrator Charles Bolden. \"This is a great day for NASA and the nation. We are grateful to the\\ncrew and their families for their dedication and professionalism.\" \"This is a very exciting day for the entire NASA\\nfamily,\" added NASA Chief Technologist David Weaver. \"The shuttle program has been a tremendous success,\\nand we are proud of the work of the men and women of the shuttle fleet. This is a day of celebration for all of us at\\nNASA.\" The STS flight was the first of two planned shuttle missions to the ISS. The second mission, STS-134, is\\nscheduled for launch in late November. \"Today’s successful docking marks the end of a long journey for the space\\nshuttle,\" noted NASA Administrator Boldin. \"It is a testament to the hard work and dedication of the astronauts\\nand engineers who have flown on these missions. It is also a tribute to the strength of the American spirit and the\\ningenuity of our nation’s scientists and engineers.\" \"We are very proud of the team that has worked so hard\\nFSD-vec: 7:30 a.m. EDT. The docking was completed at 8:15 a.m. EDT, NASA officials said. \"This is a great\\nday for all of us,\" said Rick Mastracchio, commander of the space shuttle program. \"We are very proud of the\\ncrew and the team that worked so hard to get this done.\" The shuttle’s arrival at the ISS marks the first time\\na U.S. spacecraft has docked with the orbiting laboratory since the end of the Apollo era in 1972. \"Today is a\\nhistoric moment for the United States and the world, and we are grateful to the astronauts and their families for\\ntheir dedication and professionalism throughout this process,\" President Obama said in a statement. \"I want to\\nthank the entire NASA team for their efforts to bring the Space Shuttle back to Earth safely, and I congratulate the\\nSTS-133 crew on their successful return to flight. This is a very special achievement for our nation, and it is a\\ntestament to the ingenuity and determination of the American people. We are all looking forward to seeing what\\nthey do next!\" The mission, which was originally scheduled to launch on May 20, will be the last of the current\\ngeneration of shutt\\nTable 19: Comparison of FSD and FSD-vec on wikinews.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='Prompt-1: <|endoftext|> A tropical depression developed near Y ap on November 11. The system intensified\\ngradually as it tracked quickly westward toward the Philippines. An anticyclone aloft\\nFSD: over the central Pacific was responsible for the development of the system. The system was located about\\n1,000 miles west of Y ape, Papua New Guinea, and about 2,500 miles east of Guam. It was moving west at\\nabout 15 mph. A tropical storm warning was issued for Y ap and the surrounding area. On November 12, the\\nsystem strengthened to a tropical cyclone. It was named Y olanda after the city in the Philippines where it formed.\\nY oland, a small town in Y apto, is located on the northern coast of the island of Y ap. The National Hurricane Center\\nsaid the storm was expected to strengthen further and become a hurricane by the time it reaches the western\\nPacific. Forecasters said Y olan will likely become a Category 1 hurricane on Friday. \"The center of this system\\nis expected to move over land near the southern Philippines on Saturday and Sunday,\" the center said. \"The\\nmaximum sustained winds are near 45 mph, with higher gusts.\" Y olanda is about 3,200 miles south of Manila.\\n\"Y olan is forecast to become a major hurricane later this week,\" the National Weather Service said. \"It is likely\\nFSD-vec: over the central Pacific was responsible for the development of the storm. The typhoon, which is named\\nafter the island of Y ape, formed in the western part of the Central Philippine Sea. It moved northwest at a speed\\nof about 10 mph (16 km/h) and then turned east-northeast. It weakened slightly during the night, but remained a\\nCategory 1 hurricane with maximum sustained winds of 115 miles per hour. Y ap is located about 2,000 miles\\nsouthwest of Manila. A Tropical Storm Warning was issued for the northernmost parts of the country, including the\\nprovinces of Cebu, Iloilo, and Bohol. Forecasters said the cyclone could bring heavy rain and strong winds to the\\nsouthern and eastern portions of the archipelago. \"We are expecting some very intense rainfall, especially in the\\nsouth and southeast,\" National Disaster Risk Reduction and Management Council (NDRRMC) Director General\\nJoel Pagdilao told ABS-CBN News. \"There will be gusty wind, so we need to be prepared for that.\" Pagdilao\\nadded that the NDRRMC is monitoring the situation closely. \"Our teams are on\\nPrompt-2: <|endoftext|> Malaysia LNG is a Liquefied Natural Gas ( LNG ) manufacturing complex located in\\nBintulu that currently contains eight LNG trains with a ninth\\nFSD: train under construction. The complex is owned by the Malaysian government and is operated by Malaysia’s\\nstate-owned Petronas. The complex was built in the early 1990s to supply LPG to the domestic market. In 2004,\\nPetrolio, a subsidiary of Petronas, acquired the LNGL project and began to produce LNB. In 2009, the government\\nof Malaysia announced that it would be building a second LGNL complex in Kuala Lumpur. This second complex,\\nwhich is currently under development, will be capable of producing up to 1.5 million tonnes of LBN per year.\\nPetronas has stated that the second facility will have a capacity of up to 2.2 million tons per annum. LNG train\\nThe first LNG train was delivered to BINTUL in April 2010. The train is capable of transporting up to 4.3 million\\nlitres of LNB per day. A second train, which was originally scheduled to be delivered in 2012, was delayed due\\nto a number of issues. In September 2012, Petromonas announced that the first train would be completed by\\nDecember 2012. However, in January 2013, the company announced that the train would not be ready\\nFSD-vec: train under construction. The facility is owned by Malaysia’s state-owned Petronas. The plant was\\nbuilt in the early 1990s and has been used to produce liquified natural gas for export. It is one of the largest\\nindustrial facilities in Southeast Asia. In addition to the four existing rail lines, there are plans to build a fifth line.\\nLiquifed-natural gasses are produced from coal, oil, or biomass. They are then transported by pipeline to the\\nterminal, where they are loaded onto tankers and shipped to other countries. Malaysia produces about 1.5 million\\ntonnes of liquid fuel per year. A new railway will be constructed to transport the gas to the port. Construction of\\nthe new train began in 2009. It is expected to be completed by 2015. The first two tracks were laid in 2010. The\\nthird track was installed in 2011. The fourth and final section of the railway was opened in 2012. The LNGL project\\nis part of the Malaysian government’s plan to increase its energy security. Petrol prices have been rising steadily\\nsince the beginning of the year. According to the Petroleum Ministry, petrol costs\\nTable 20: Comparison of FSD and FSD-vec on wikitext.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-02-28T02:26:49+00:00', 'author': '', 'keywords': '', 'moddate': '2024-02-28T02:26:49+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'https://arxiv.org/pdf/2305.12675.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='Prompt-1: The person slowly backed out of the snag. Olivia was expecting either a kid or a deranged hermit, so\\nshe was startled to see a young woman,\\nFSD: maybe in her early twenties, standing there. She was dressed in a black dress, with a white blouse and\\nblack pants. Her hair was tied back in a ponytail, and she had a small, round face. \"Hello, Olivia,\" the woman said.\\n\"I’m Sarah. I’m here to help you.\" Olivia’s eyes widened. \"Y ou’re Sarah?\" Sarah nodded. \"Y es. I’ve been looking\\nfor you for a while. Y ou’re the one who saved my life.\" She smiled. \"Thank you. It was my pleasure.\" Oblivious to\\nthe fact that she’d just met a stranger, Olivia walked over to Sarah and hugged her. Sarah returned the hug, and\\nOlivia felt her heart swell with gratitude. \"I’ve got to go,\" Sarah said. As Olivia stood up, she noticed that Sarah\\nhad left her purse behind. \"Where’s your purse?\" Olivia asked. Sarah shrugged. \"It’s in my room. I’ll be back\\nsoon.\" Olivetti The next day, Sarah came back to Olivia’s room. \"Y ou look tired,\" she said. I know, I\\nFSD-vec: maybe in her early twenties, standing there. She had long, dark hair, and wore a black dress with a\\nwhite collar. Her eyes were closed, but they looked like those of a child. \"Hello,\" said the girl. \"I’m Emily. I’ve been\\nwaiting for you.\" Olivia’s heart sank. Emily smiled. She took off her glasses and wiped her eyes. The two of them\\nwalked into the house. They sat down on the couch. It was quiet. Olivia felt as if she’d been transported back to\\nthat night. But this time, it wasn’t just because of the silence. There was something else. Something different. The\\nair was warm. And, somehow, there was a sense of peace. Olivia couldn’t quite put her finger on it. Maybe it was\\nthe fact that the room was empty. Or maybe, just maybe, it was because there was someone sitting across from\\nher. A tall, thin man. He didn’t look at her. Instead, he stared at the floor. His face was expression\\nPrompt-2: The trip ended far too soon; I was a bit confused as we pulled up to a small beach cottage on the far\\nside of the inlet. The whit\\nFSD: ewashed walls were covered in graffiti, and the windows were boarded up. I asked the owner if he knew\\nwhere we could find a place to stay, but he didn’t seem to know either. We decided to walk back to the car and\\nhead back out to the inlets. We walked along the beach for a while, until we came across a large group of people\\nsitting on a bench. They were all dressed in white, and they were talking about something. It was hard to make\\nout what they were saying, but I could tell they had a lot of energy. I asked one of the guys if I could join them, and\\nhe said sure. He led me to a spot on the beach where he had set up a tent. As we sat down, I noticed that the guy\\nwas wearing a white shirt with a black tie. \"What’s your name?\" I asked. \"I’m John,\" he replied. He was tall, with\\ndark hair and a beard. He was also wearing jeans and a t-shirt. John was an older man, probably in his late 50s\\nor early 60s. He had dark skin, a thin mustache, and was balding. As we\\nFSD-vec: ewashed walls were covered in graffiti, and the windows were boarded up. We walked inside, and\\nfound a large, empty room. \"I’m sorry, but there’s nothing here,\" said my guide. We sat down at the table, and he\\ntold us about his life. He had been born in Mexico, but moved to the United States when he was five. His parents\\ndivorced, and he lived with his mother. He worked as a cook, and then became a security guard. When that job\\ndried up, he started working for a construction company. When the recession hit, things got worse. The economy\\nwas in shambles, and many people lost their jobs. His boss fired him, and sent him to live with his grandmother.\\nThat’s when everything changed. A few months later, his father died. It was hard for him to cope. So, after living\\nwith his grandparents for a while, he decided to move back to Mexico. But, before he left, he took some photos.\\nOne day, while walking home, he saw a man who looked like him. They talked, and eventually, they agreed to\\nmeet\\nTable 21: Comparison of FSD and FSD-vec on book.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = text_docs + pdf_docs"
      ],
      "metadata": {
        "id": "aArQ05cZ2357"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import split\n",
        "spiltter = RecursiveCharacterTextSplitter(chunk_size = 500 ,chunk_overlap = 100)\n",
        "split_docs = spiltter.split_documents(all_docs)"
      ],
      "metadata": {
        "id": "C_UNBReK3Uf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_embedder = HuggingFaceBgeEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "hf_embeddering = hf_embedder.embed_documents([docs.page_content for docs in split_docs])\n"
      ],
      "metadata": {
        "id": "8Sh3DveA5Knk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hugging_face lenght : \" , len(hf_embeddering))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy8nTIEj5mFu",
        "outputId": "c5b4702d-8cd3-41da-90e6-62e17b5f7519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hugging_face lenght :  229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "chroma_store = Chroma.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=hf_embedder,\n",
        "    persist_directory=\"chroma_db\"\n",
        ")"
      ],
      "metadata": {
        "id": "M5nfdMJn6X9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Langchain?\"\n",
        "result = chroma_store.similarity_search(query)\n",
        "for i,res in enumerate(result):\n",
        "   print(f\"\\nResult {i+1}:\\n{res.page_content[:300]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiI2zAy_7YEe",
        "outputId": "adf69367-f006-4571-e656-78acfe14f117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result 1:\n",
            "LangChain is a framework for developing applications powered by language models. It enables the chaining of components like prompt templates, language model calls, and output parsers in a flexible and easy-to-use way.\n",
            "\n",
            "LangChain supports document loading, splitting, embedding, and storage using vect\n",
            "\n",
            "Result 2:\n",
            "3. Background\n",
            "3.1. Language Models\n",
            "An LM is a probability distribution over token se-\n",
            "quences. Given a sequence x1:t = x1, x2, . . . , xt\n",
            "of length t, LM assigns a probability p(x1:t) to the\n",
            "sequence, which is usually decomposed in an au-\n",
            "toregressive fashion: p(x1:t) =Qt\n",
            "i=1 p(xi|x<i).\n",
            "N-gram Langu\n",
            "\n",
            "Result 3:\n",
            "7. References\n",
            "BigScience. 2023. Bloom: A 176b-parameter open-\n",
            "access multilingual language model.\n",
            "Tom B. Brown, Benjamin Mann, Nick Ryder,\n",
            "Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\n",
            "wal, Arvind Neelakantan, Pranav Shyam, Girish\n",
            "Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n",
            "Herbert-Voss, Gretche\n",
            "\n",
            "Result 4:\n",
            "First, all the operations (i.e., construction, predic-\n",
            "tion, and update) associated with an n-gram model\n",
            "add little computational overhead. Second, the ef-\n",
            "fectiveness and efficiency ofn-gram LM is scalable\n",
            "across different prefix lengths.\n",
            "Construction and Update Given an input\n",
            "prompt x1:l, the n-gr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_store.persist()\n",
        "print(\"Chroma DB persisted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sQBdxXS_mu8",
        "outputId": "e6490389-c23c-4eb4-a375-e27f2308a3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma DB persisted!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-fbf7ec0e3050>:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  chroma_store.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_chroma = Chroma(persist_directory=\"chroma_db\", embedding_function=hf_embedder)\n",
        "print(\"Chroma DB loaded with\", len(loaded_chroma.get()[\"documents\"]), \"documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeIYhbDqB-NT",
        "outputId": "e542d4c0-9a2b-4d23-c7f5-6379e8512942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chroma DB loaded with 229 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-952cdc8cb6e4>:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  loaded_chroma = Chroma(persist_directory=\"chroma_db\", embedding_function=hf_embedder)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NiX6idFRCDLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}